@inproceedings{Liu2011,
abstract = {Boosting is a well known machine learning technique used to improve the performance of weak learners and has been successfully applied to computer vision, medical image analysis, computational biology and other fields. A critical step in boosting algorithms involves update of the data sample distribution, however, most existing boosting algorithms use updating mechanisms that lead to overfitting and instabilities during evolution of the distribution which in turn results in classification inaccuracies. Regularized boosting has been proposed in literature as a means to overcome these difficulties. In this paper, we propose a novel total Bregman divergence (tBD) regularized LPBoost, termed tBRLPBoost. tBD is a recently proposed divergence in literature, which is statistically robust and we prove that tBRLPBoost requires a constant number of iterations to learn a strong classifier and hence is computationally more efficient compared to other regularized boosting algorithms in literature. Also, unlike other boosting methods that are only effective on a handful of datasets, tBRLPBoost works well on a variety of datasets. We present results of testing our algorithm on many public domain databases along with comparisons to several other state-of-the-art methods. Numerical results depict much improvement in efficiency and accuracy over competing methods.},
author = {Liu, Meizhu and Vemuri, Baba C.},
booktitle = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
doi = {10.1109/CVPR.2011.5995686},
isbn = {9781457703942},
issn = {10636919},
pages = {2897--2902},
pmid = {24348005},
title = {{Robust and efficient regularized boosting using total bregman divergence}},
year = {2011}
}

@phdthesis{liu2011total,
  title={Total Bregman divergence, a robust divergence measure, and its applications},
  author={Liu, Meizhu},
  year={2011},
  school={University of Florida}
}

@inproceedings{le2007bundle,
  title={Bundle methods for machine learning},
  author={Le, Quoc V and Smola, Alex J and Vishwanathan, SVN},
  booktitle={Advances in neural information processing systems},
  pages={1377--1384},
  year={2007}
}

@inproceedings{radlinski2006minimally,
  title={Minimally invasive randomization for collecting unbiased preferences from clickthrough logs},
  author={Radlinski, Filip and Joachims, Thorsten},
  booktitle={PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE},
  volume={21},
  number={2},
  pages={1406},
  year={2006},
  organization={Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}
}

@incollection{ruckert2011transfer,
  title={Transfer learning with adaptive regularizers},
  author={R{\"u}ckert, Ulrich and Kloft, Marius},
  booktitle={Machine Learning and Knowledge Discovery in Databases},
  pages={65--80},
  year={2011},
  publisher={Springer}
}

@misc{Pan2010,
abstract = {A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.},
author = {Pan, Sinno Jialin and Yang, Qiang},
booktitle = {IEEE Transactions on Knowledge and Data Engineering},
doi = {10.1109/TKDE.2009.191},
isbn = {1041-4347 VO - 22},
issn = {10414347},
keywords = {Transfer learning,data mining.,machine learning,survey},
pages = {1345--1359},
title = {{A survey on transfer learning}},
volume = {22},
year = {2010}
}